
// adds commentary and GPL to first page of doxygen-erated HTML browser

/*! \mainpage PISM C++ Class Browser
<CENTER>
<i>
for PISM trunk revision \verbinclude revision.tex
</i></CENTER>

This searchable \e Browser shows the C++ class (object) structure of
PISM, the <a href="http://www.pism-docs.org/">Parallel Ice Sheet Model</a>.  It
shows \em all the \em classes, \em inheritance structure, source \em files,
and \em class \em members (variables and methods) in the PISM source code.
It was automatically generated by <a href="http://www.doxygen.org/">doxygen</a>
from comments in the PISM source code.  It is the source of PISM technical
information, including that needed to extend PISM functionality.

For a complete list of classes, click on the 
<a href="annotated.html">Classes tab</a> at the top of the page.  Some entry
points into the PISM classes are:
  - IceModel
  - IceModel::init()
  - PISMClimateCoupler
  - IceGrid
  - IceModelVec

The <a href="pages.html">Related Pages tab</a> at the top links to 
overview pages for some aspects of PISM design.  You will also find a
\ref std_names "table of standard names used by PISM", a
\ref config "table of configuration parameters for PISM", and a \ref references
"list of References".

This doxygen source \e Browser can be regenerated in any source code release.
Just do <tt>" cd doc/ && make browser "</tt> and view <tt> doxy/html/index.html</tt>

\section other Other PISM documentation

\li The website <a href="http://www.pism-docs.org/">www.pism-docs.org</a> is the
\e root for all PISM information.

\li You can \e get \e help, send comments, or ask for new features by email to
help@pism-docs.org.

\li To \e download the PISM source code see https://gna.org/projects/pism/.  
Use of <a href="http://subversion.tigris.org/">subversion</a>
to download is strongly recommended because, even for the stable branches of
the code, bug fixes are easy to distribute by <tt>" svn update "</tt>.

\li For help \e installing PISM, see the
<a href="http://www.pism-docs.org/">PISM <i>Installation Manual</i></a>.

\li For help \e using PISM, see the
<a href="http://www.pism-docs.org/">PISM <i>User's Manual</i></a>.  The 
\e User's \e Manual should suffice for understanding PISM unless one wants to 
modify it or couple another model to it.

\li There is also a
<a href="http://svn.gna.org/viewcvs/pism/"><i>source code repository browsing tool</a></i>
at the <a href="https://gna.org/projects/pism/">PISM download site</a>.  It
shows changes between source code revisions.

<i>
Copyright (C) 2008-2010 Ed Bueler and Constantine Khroulev.  

This document is part of PISM.  PISM is free software; you can
redistribute it and/or modify it under the terms of the GNU General Public License as published
by the Free Software Foundation; either version 2 of the License, or (at your option) any later
version.  PISM is distributed in the hope that it will be useful, but WITHOUT ANY
WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details.  You should have
received a copy of the GNU General Public License along with PISM; see \c COPYING in the PISM
source directory; if not, write to the
Free Software Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA  02110-1301  USA
</i>

*/

/*!
\page pismsourceID An overview of C++ classes in the PISM source code

\section overviewID Overview

FIXME!

PISM is implemented as a collection of C++ object classes.  There are five basic 
classes to know about, the sources for each of which are in \c pism/src/base/:
  - IceGrid.  Describes the computational box and grid, and the parallel layout
       of the grid.  For instance, which processor owns which part of the grid is stored in an
       instance of IceGrid.
  - IceModel.  Contains myriad data structures, flags, PETSc \c Vecs (below), and 
       most of the numerical methods of PISM.  The most important class.  An instance of 
       IceModel has data members including an instance of IceGrid, many instances of IceModelVec,
       many instances of MaterialType, and an instance of NCTool. 
  - IceModelVec.  Abstracts the idea of a scalar field distributed across the PISM grid.  Elements of 
       this class can read and write themselves to NetCDF files, for instance.  The three dimensional 
       case, the derived class \c IceModelVec3, can hand back a requested column or stencil, for instance.
  - MaterialType.  Various materials are derived from the class \c MaterialType which merely defines 
       a couple of physical constants.  \c IceType is a derived class, but still an abstract class.
       It defines the physical properties of ice as a material.  Concrete classes derived from \c IceType
       are \c ThermoGlenIce (which uses the EISMINT constants) and \c GKIce as well as \c HybridIce, 
       which implements the Goldsby-Kohlstedt flow law [\ref GoldsbyKohlstedt].  

       @note Actually, it is 
       difficult or impossible to implement a complete Goldsby-Kohlstedt \c IceType because the effective
       viscosity (inverse) form of the constitutive relation is required for computation of SSA velocity 
       fields. \c HybridIce is Goldsby-Kohlstedt ice in SIA regions and Glen ice in other regions.

       There is a basal material type \c BasalType which can be plastic or linearly viscous.  Similarly, 
       there are \c BedrockType and \c OceanType types which define associated physical constants.
  - NCTool.  Collects the methods which read and write and regrid the actual PETSc 
       \c Vecs in PISM.  These methods do nontrivial parallel data transfers.

Additional classes are \c Data1D and \c LocalInterpCtx, among others.
 
\section icemodelID Description of IceModel

The methods for \c IceModel are many.  In addition to numerics, they initialize the model 
from input data files.  (Note several derived classes of \c IceModel initialize from 
formulas describing simplified geometry experiments or exact solutions.)  The methods
in \c IceModel read user options, allocate arrays in a distributed manner under PETSc, 
control diagnostic viewers, run the adaptive time-stepping mechanism, report the state 
of PISM to the user, and write out the model state to files.

A derived class of \c IceModel called \c IceCompModel is used for verification.  It has additional 
structures which allow compensatory sources and compute initial conditions from, and errors relative to,
known exact solutions [\ref BLKCB, \ref BBL, \ref BB].  Another derived class used for verification is 
\c IceExactSSAModel, which implements the exact solution described in [\ref SchoofStream].

Other derived classes of \c IceModel are \c IceEISModel, \c IceGRNModel, and \c IceROSSModel.  
These correspond to the EISMINT II simplified geometry experiments [\ref EISMINT00] , and the 
EISMINT-Greenland ice sheet and EISMINT-Ross ice shelf models described in the \e User's \e Manual.

There are five established drivers which just call constructors and destructors for an instance 
of \c IceModel, or a derived class thereof, and tell the instance to run itself.  These are in the 
source files \c pgrn.cc, \c pismd.cc, \c pismr.cc, \c pisms.cc, and \c pismv.cc, corresponding to 
the executables with the same names.  The drivers \c pismr.cc and \c pismd.cc only use the base 
class \c IceModel, so one is required to initialize from a PISM model state NetCDF file with 
\c -i or from an incomplete model state NetCDF file using \c -boot_from.

The different derived classes generally invoke the same numerical procedures to handle the various 
partial differential equations of the continuum model.  In the case of \c IceCompModel and
\c IceExactSSAModel, this fact is at the heart of the verification mechanism.

*/

/*!
\page petscID PETSc: An overview for PISM users

FIXME!:  gloss why a lot of this is now buried in stable0.3; derived class writers now need minimal knowledge of PETSc

FIXME:   instead, what is important to know is that solving the SSA in parallel
is relatively deep technology, and the solution depends (through preconditioner
issues) on number of processors

\section intro Introduction
The PETSc library [\ref petsc-user-ref] provides essential support for distributed 
arrays and linear solvers in a parallel computing environment.  ``PETSc" stands for 
``Portable, Extensible Toolkit for Scientific Computation."  It is a suite of data structures 
and routines in C for the scalable parallel solution of partial differential equations and their 
scientific applications.  Large parts of PETSc relate especially to finite-difference-type regular, 
rectangular grids but PETSc has been used for unstructured grids as well.

PETSc employs the MPI standard for all message-passing communication but it is deliberately at a 
higher level of abstraction than MPI.  We find that using PETSc protects the programmer from 
explicit consideration of message passing about 90% of the time.

Documentation for PETSc is at 

http://www.mcs.anl.gov/petsc/.

PISM is a C++ program and PETSc is a C library.  All PISM calls to PETSc use the PETSc C API, not the C++ API.


\subsection petsctypesID PETSc types

Most important variables in a PETSc program are of these types:
  - \c DA
  - \c Vec
  - \c KSP

In fact most of the PETSc types merely declare pointers, but these types should be regarded 
as abstract data types.  Variables must be created with calls to functions like \c DACreate2d(), 
\c VecCreate(), etc., and destroyed when no longer needed.


\subsection distributedID Distributed arrays and vectors

PETSc has an abstract date type called a Distributed Array (type \c DA).  \c DAs contain 
information about the grid and stencil.  They set up \e ghosted values which are intended to
duplicate the values on neighboring processors so communication can be done in big batches.

Vectors (type \c Vec) are created \e using a \c DA with \c DAVecCreate() and similar 
procedures.  These vectors are distributed across the processors according to the information
in the \c DA.  Just for emphasis: <i>PISM distributes itself across the processors by calling
PETSc procedures which create a \c DA</i>.

There are two parameters of note when creating a \c DA: \e stencil \e type and 
\e stencil \e width.  

The stencil types are \c DA_STENCIL_STAR and \c DA_STENCIL_BOX.  
They are generalizations of the five point and nine point stencils typical of two-dimensional 
discretizations of second order partial derivatives.  Using \c DA_STENCIL_STAR means
that ghosted points are needed only along the coordinate axes, while \c DA_STENCIL_BOX 
indicates that ghosted points are needed in the box-shaped %region surrounding each point, 
and thus each processor.  (On processor \e N, information owned by a neighboring processor 
but duplicated onto \e N is called \e ghosted.  A picture is worth a thousand words here, so 
find the appropriate picture in the PETSc manual!)  We only need \c BOX style stencils when gradient 
terms must be evaluated on a staggered grid (\e h in SIA regions and \f$\bar{u},\bar{v}\f$
in the computation of effective viscosity in SSA regions).  Keeping all other 
two-dimensional vectors on a \c STAR type stencil would reduce the necessary communication
slightly, but for now all two dimensional arrays are based on \c BOX.

The stencil width indicates how many points in each direction will be needed.  We never 
need a stencil width greater than 1.

The three dimensional distributed arrays are distributed across processors in the same way
as two-dimensional arrays, in the sense that <i>each column of the ice (or bedrock in that 
thermal model) is owned by exactly one processor</i>.  From the point of view of parallelization,
our problem is two-dimensional.  All three-dimensional arrays have stencil type \c STAR in the 
horizontal directions.

One point of confusion (we admit ...) is the redefinition of the \f$x,y,z\f$ axes. Contrary 
to the PETSc default, our \f$z\f$ axis changes most rapidly through memory while the
\f$x\f$ axis changes most slowly.  The desirable consequence, however, is that our C
arrays are addressed as <tt>u[i][j][k]</tt> where <tt>i,j,k</tt> are the coordinate 
indices in the directions \f$x,y,z\f$ respectively.


\subsection davecsID Accessing the processor's part of a DA-managed Vec

\c DA -based vectors must be accessed by a call to \c DAVecGetArray() before the access and
a call to \c DAVecRestoreArray() after.  This just means that one gets a valid pointer 
to the actual memory, for the abstract data type \c Vec.  The point has type 
<tt>double**</tt> for a 2-dimensional array and type <tt>double***</tt> for a 3-dimensional 
array.

The resulting pointer can be addressed using normal (to the extent that C is "normal" in 
this regard ...) multidimensional array indexing.  Furthermore, the indices themselves 
allow one to pretend that the given processor is addressing the entire array, even though only a
chunk is stored on the local processor.  <i>No message passing occurs here.</i>  Instead, the crucial
idea is that the \c DA knows what are the valid index ranges for each processor.  That's why 
essentially all loops over two dimensional arrays in PISM look like this:

\code
  for (PetscInt i = grid.xs; i < grid.xs + grid.xm; ++i) {
    for (PetscInt j = grid.ys; j < grid.ys + grid.ym; ++j) {

    ... H[i][j] ...
    
    ... ubar[i][j] ...
    
    ... T[i][j][k] ...

    }
  }
\endcode

The integers \c grid.xs, \c grid.xm, \c grid.ys, \c grid.ym, are just the numbers that come
from a call to \c DAGetLocalInfo(), just after the \c DA is created.


\subsection localglobalID "Local" versus "global" Vecs

PETSc \c DA -based vectors can be "local" or "global".   (This weird PETSc language should
probably be translated as "has ghosts" or "sans ghosts".)  "Local" vectors include 
space for the ghosted points.  That is, when \c DAVecGetArray() is called, the resulting 
array can be indexed on all of the local portion owned by the processor, \e plus the 
ghosted points.  The processor owns \e copies of the values at grid points owned by the neighboring
processors, according to the stencil type and stencil width in the \c DA.  The processor should 
treat these ghost values as "read-only", because of what happens at the communication stage 
(which we describe in a moment).

\e Example.  We create a local \c Vec \c v which we want to have \c STAR type stencil.  Assuming 
\c da is \c DA created with \c DA_STENCIL_STAR then we do this to create \c v and get an array 
pointer \c arr into the local portion of it:
\code
   DACreateLocalVector(da, &v);
   DAVecGetArray(da, v, &arr);
\endcode
Now the processor can access <tt>arr[i][j]</tt> for every 
\code 
   grid.xs <= i <= grid.xs + grid.xm - 1
\endcode
and
\code 
   grid.ys <= j <= grid.ys + grid.ym - 1
\endcode
But this is just the regular (owned by the processor) portion.  In addition, all of these 
are valid for every \c i and \c j in the above ranges:
\code
   arr[i+1][j],   arr[i-1][j],   arr[i][j+1],   arr[i][j-1].
\endcode
(\e If the \c da were of type \c DA_STENCIL_BOX then, in addition 
\code
   arr[i+1][j+1],   arr[i-1][j+1],   arr[i+1][j-1],   arr[i-1][j-1].
\endcode
would all be valid.)  Once the processor is done with modifying its portion of \c v (i.e. not including 
the ghosts, although it may read them) then we would want to update the ghost values so that
all processors agree on the values at those grid points:
\code
   DAVecRestoreArray(da, v, &arr);
   DALocalToLocalBegin(da, v, INSERT_VALUES, v);
   DALocalToLocalEnd(da, v, INSERT_VALUES, v);
\endcode
(Note we release the array pointer before the communication stage, although that may not be
essential.)

\c DALocalToLocalBegin() and then \c DALocalToLocalEnd() are called to update (communicate) the 
ghosted values before the next stage at which they will be needed.  This pair of commands 
perform a stage of actual (MPI) message passing.  Only enough values are passed around to 
update the ghosts.  The size of the messages is relatively small, and <i>latency is more important
(in slowing performance) than bandwidth</i>.  The significance of \c STAR versus \c BOX is not
in the size of the extra memory, but in the extra messages which must be passed to the "diagonal"
neighboring processors.

Global vectors do not hold ghosted values.  Certain array operations are more appropriate to
these kind of vectors, like viewing arrays in graphics windows.  (Of course, viewing an array 
distributed across all the processors requires message passing.  I think PETSc sends all the
values to processor 0 to display then.)  In any case, when ghosts are not needed there is
no need to allocate space for them, and a "global" \c Vec is appropriate.  Local vectors
typically need to be mapped to global vectors before viewing or using in a linear system, but
this is an entirely "local" operation which does not require message passing (\c DALocalToGlobal()).


\subsection linearsysID Solving linear systems

PETSc is designed for solving large, sparse systems in parallel.  Iterative methods are 
the name of the game and especially Krylov subspace methods such as conjugate gradients and 
GMRES [\ref TrefethenBau, \ref Saad].  For consistency, all methods use the same 
Krylov-subspace-method-with-preconditioner \c KSP interface, even though some are really direct 
methods.

The place in PISM where this is already used is in the solution of the linearized SSA velocity
problem.  See the documentation for IceModel::velocitySSA() and IceModel::assembleSSAMatrix() 
in this \e Manual.

One starts by declaring an object of type \c KSP; in PISM this is done in IceModel::createVecs().
Various options can be set by the program, but, if the program calls \c KSPSetFromOptions() 
before any linear systems are solved, which PISM does, then user options like 
\c -ksp_type, \c -pc_type, and \c -ksp_rtol can be read to control (respectively) 
which Krylov method is used, which preconditioner is used, and what relative 
tolerance will be used as the convergence criterion of the Krylov solver.  The default is
GMRES(30) with ILU preconditioning.

(As a general mechanism, PETSc has an options database which holds command line options.
All PISM options use this database.  See IceModel::setFromOptions().)

To solve the system, a matrix must be attached to the \c KSP.  The first time
\c KSPSolve() is called, the matrix will be factored by the preconditioner and reused
when the system is called for additional right hand sides.  In the case of PISM and the 
solution of the balance of momentum equations for the SSA, the matrix changes because the
equations are nonlinear (because the effective viscosity depends on the strain rates).

The default matrix format is similar to the \e Matlab \c sparse format.  Each processor
owns a range of rows.  Elements in matrices and vectors can be set using \c MatSetValues()
and \c VecSetValues().  These routines use a "global" indexing which is not based on the regular
grid and stencil choices in the \c DA.  One can set any value on any processor but a lot of
message passing has to occur; fortunately PETSc manages that all.  Values are
cached until one calls \c MatAssemblyBegin() followed by \c MatAssemblyEnd() to
communicate the values.

In fact, in the SSA velocity computation, the matrix, the solution and right hand side 
vectors are not DA based.  The solution is moved into DA based arrays (for vertically-averaged 
velocity) by IceModel::moveVelocityToDAVectors() and IceModel::broadcastSSAVelocity().

The vector (field) components are interlaced and distributed.  This seemed to be the
most straightforward method to solve the system (as opposed to using more advanced
features intended for multiple degrees of freedom on DA based vectors).  This also allows
the matrix to have an optimal parallel layout.


\subsection additionalpetscID Additional PETSc utility functions

The \c PetscViewer interface allows PETSc objects to be displayed.  One can "view"
a \c Vec to a graphical (X) window, but the "view" can be saving the \c Vec to a binary
file on disk, in plain text to the terminal (standard out), or even by sending to a running
instance of \e Matlab.  (This last capability is not used yet in PISM.)

FIXME  option -d is gone

When viewing \c Vec s graphically in multiprocessor jobs, the display may have to be 
set on the command line, for instance as <tt>-display :0</tt> or similar; this must be given 
as the final option.  For example,
\code
   mpiexec -n 2 pismv -test C -Mx 101 -My 101 -Mz 31 -y 1000 -d HT -display :0
\endcode
allows a two processor run to view (graphically) both the ice thickness (<tt>-d H</tt>)
and the ice temperature (<tt>-d T</tt>).

Quite ellaborate error tracing and performance monitoring is possible with PETSc.  All
functions return \c PetscErrorCode which should be checked by the macro \c CHKERRQ().
Therefore runtime errors normally print sufficient traceback information.

If this information is not present (because the error did not get traced back), you 
may need to use a debugger which is accessible with the command line (PETSc) options 
\c -start_in_debugger and \c -on_error_attach_debugger.  

Option \c -log_summary is useful to get some diagnostics written to the terminal.

*/
 

